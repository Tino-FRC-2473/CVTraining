{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_O9Q1c1hc-I_"
      },
      "source": [
        "# Part 2: Color\n",
        "\n",
        "Now that we have the basics of image manipulation under our belt, let's take things to the next dimension: color images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg7jpZ6JBRZQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import skimage.color\n",
        "import cv2\n",
        "\n",
        "from mpl_toolkits import mplot3d\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import collections\n",
        "from mpl_toolkits.mplot3d import art3d\n",
        "\n",
        "print(\"Import done!\")\n",
        "\n",
        "! git clone https://github.com/Tino-FRC-2473/CVTraining repo_files"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GzvSCPCldQZt"
      },
      "source": [
        "## 2.1 What is color, really?\n",
        "\n",
        "In elementary school, we are taught that colored light is made up of three primary colors that we can mix together to make any other color. This is often shown in a kind of color venn diagram or color wheel\n",
        "\n",
        "![Color wheel](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/AdditiveColor.svg/240px-AdditiveColor.svg.png)\n",
        "\n",
        "In physics, you probably learned that the color of light is dependent on the frequency of light waves. This is usually demonstrated by a prism breaking light into a continuous rainbow as different frequencies of light are bent differently.\n",
        "\n",
        "![Visible Spectrum](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Linear_visible_spectrum.svg/500px-Linear_visible_spectrum.svg.png)\n",
        "\n",
        "But there's a bit of a problem here. Just putting some red and green photons together doesn't change their frequency to yellow. And a cyan photon can't magically split apart into a blue and green photon when it enters a camera. And why are red, green, and blue so special?\n",
        "\n",
        "So we come to the next generation of color theory: the tristimulus model. The light sensing cone cells in our eyes have three different kinds of pigments. Each pigment produces a different level of response when it absorbs different frequencies of light.\n",
        "\n",
        "![Cone color response](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Cones_SMJ2_E.svg/500px-Cones_SMJ2_E.svg.png)\n",
        "\n",
        "Our brains then combine the level of response from each of the three classes of cone cells to estimate what frequency of light we're looking at. For camera and display makers, this is great news since if we can just stimulate these three classes of cells independently, we can simulate any color we want. The peaks of the three responses are at red, green, and blue. So we have our first color system, RGB.\n",
        "\n",
        "Note, however, that the three cells aren't completely independent. In other words, we can't pick frequencies that only stimulate one of the three cell types because their response curves overlap. So we can mix red and green light to look *similar* to yellow light, but not *exactly the same* as true yellow light.\n",
        "\n",
        "On top of this, the brain does all kinds of processing to estimate color that we just can't model with three numbers. For instance, the color brown doesn't appear on any of these models, because it is an artifact of the way our brains judge relative brightness. If you're interested in learning more, the famous book [_Interactions of Color_](hhttps://sccl.bibliocommons.com/item/show/389841118) by Josef Albers lays out a series of exercises to explore how we perceive color with an eye toward graphic design and art.\n",
        "\n",
        "![Albers](https://collectionapi.metmuseum.org/api/collection/v1/iiif/737721/1626557/restricted)\n",
        "\n",
        "We're only scratching the surface here, but I hope this gives a bit of background into where RGB comes from and the underlying complexity of color representation.\n",
        "\n",
        "![XKCD](https://imgs.xkcd.com/comics/color_models.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d6U1m1Ponz1Y"
      },
      "source": [
        "## 2.2 Color images\n",
        "\n",
        "So to summarize, we need three values in order to represent color as humans see it. So a color image has three values for each pixel. These are usually referred to as channels, so for RGB we have a red channel, a green channel, and a blue channel. \n",
        "\n",
        "Now remember, OpenCV does things a bit backward, so we're going to need to swap the order of the color channels from BGR to RGB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23ZBZP0CdIlO"
      },
      "outputs": [],
      "source": [
        "def bgr_to_rgb(image):\n",
        "  return image[:,:,::-1]\n",
        "\n",
        "! curl https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/AdditiveColor.svg/240px-AdditiveColor.svg.png -o rgb_diagram.png\n",
        "! curl https://www.nasa.gov/sites/default/files/thumbnails/image/pillars_of_creation.jpg -o hubble.jpg\n",
        "\n",
        "color_diagram_image = bgr_to_rgb(cv2.imread('rgb_diagram.png'))\n",
        "plt.imshow(color_diagram_image)\n",
        "plt.show()\n",
        "\n",
        "hubble_image = bgr_to_rgb(cv2.imread('hubble.jpg'))\n",
        "plt.imshow(hubble_image)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7HXUMUznID7L"
      },
      "source": [
        "### Exercise\n",
        "When looking at a numpy array representing a color image, the color channels make up a third dimension. So instead of a one-dimensional scalar brightness like we had in grayscale images, we have three values representing each color channel. Let's split these channels apart and take a look. I've written the code to isolate the red channel. Fill in the code for isolating the green and blue channesl."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGm4GsErVlTJ"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.title(\"Red only\")\n",
        "red_only_image = color_diagram_image.copy()\n",
        "# Zero out Green channel\n",
        "red_only_image[:,:,1] = 0\n",
        "# Zero out Blue channel\n",
        "red_only_image[:,:,2] = 0\n",
        "plt.imshow(red_only_image)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Green only\")\n",
        "green_only_image = color_diagram_image.copy()\n",
        "# WRITE CODE HERE\n",
        "# Zero out Red channel\n",
        "\n",
        "# Zero out Blue channel\n",
        "\n",
        "# END CODE\n",
        "plt.imshow(green_only_image)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Blue only\")\n",
        "blue_only_image = color_diagram_image.copy()\n",
        "# WRITE CODE HERE\n",
        "# Zero out Red channel\n",
        "\n",
        "# Zero out Green channel\n",
        "\n",
        "# END CODE\n",
        "\n",
        "plt.imshow(blue_only_image)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m7-kiKs5KAI-"
      },
      "source": [
        "##2.3 Color Spaces\n",
        "\n",
        "While RGB matches how humans see the world, it often isn't very intuitivite or efficient to work with. In particular, it's often hard to separate what color a pixel is from how bright a pixel is. Is the blue channel high in this pixel because it's a picture of a blue object, or is it just a very bright pixel so all three channels have large values?\n",
        "\n",
        "To solve these problems, there are a number of different schemes for representing color information known as \"color spaces\" each optimized for different tasks. Note that since color information requires three channels, most useful color spaces are also three dimensional. However what each dimension/channel represents can be remapped and redesigned in creative ways."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6CTBgPzShXND"
      },
      "source": [
        "### 2.3.1 Hue-Saturation-Value (HSV)\n",
        "\n",
        "![HSV Cylinder](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/HSV_color_solid_cylinder.png/640px-HSV_color_solid_cylinder.png)\n",
        "\n",
        "One very common color space is the Hue-Saturation-Value color space, commonly abbreviated HSV. This color space is designed to present color in a way that more closely matches how we intuitiviely think about color. First, H represents the \"hue\" of a color as an angular value from 0-360ยบ. This fits well with the idea of a color circle. Saturation (S) roughly corresponds to how intense or saturated a color is. Finally, value (V) is intended to represent the brighness of a color, with 0 being black and 1 being as bright as possible.\n",
        "\n",
        "You'll often seen HSV being used in user interfaces for \"color picker\" tools. These components match well to the way we intuitively talk about color, such as \"bright green\" or \"dark blue.\" However, there are some drawbacks for using HSV for computer vision. For instance, the difference between saturation and value is a bit ambiguous. Near the center of the HSV cylinder shown above, there's a big region of pale white colors that look about the same, even though their value and saturation are quite different. After all, what's the difference between a low-saturation or faded color, and a bright white? These ambiguities make HSV tricky to use in computer vision algorithms that need to work across different lighting conditions.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cgwUdRzA7opg"
      },
      "source": [
        "### Exercise\n",
        "Let's have a little fun with HSV. One popular kind of filter simply boosts or decreases the saturation of a image. Let's implement that now. [Skimage's color module](https://scikit-image.org/docs/dev/api/skimage.color.html) lets us covert between color spaces. The second argument is a constant describing the source and destination color space names.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km63DIGyPY58"
      },
      "outputs": [],
      "source": [
        "def edit_saturation(rgb_image, factor):\n",
        "  # Convert RGB to HSV\n",
        "  hsv_image = skimage.color.rgb2hsv(rgb_image)\n",
        "  # Multiply the saturation channel of hsv_image by factor\n",
        "  # WRITE CODE HERE\n",
        "\n",
        "  # END CODE\n",
        "  # Convert back to RGB\n",
        "  return skimage.color.hsv2rgb(hsv_image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yn5NFfBQN-9"
      },
      "source": [
        "Now let's use this function to adjust the saturation of a test image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsHHqgexQTb_"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.title(\"Original\")\n",
        "plt.imshow(hubble_image)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Saturation boost\")\n",
        "plt.imshow(edit_saturation(hubble_image, 2))\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dGqCnc8cQzxB"
      },
      "source": [
        "###2.3.2 Luminance and chrominance, YUV/YCbCr\n",
        "\n",
        "This problem of isolating lighting information from color information comes up in a wide range of different applications, and so it is no surprise that there are other color spaces that perform this kind of separation. In particular, the closely related YUV and YCbCr color spaces were designed to explicitly separate brightness from color. These color spaces were originally designed when the first color televisions were being developed, as a way to separate a brightness-only signal for older black and white televisions from a color television signal. These days they are commonly used in computer vision and digital video processing applications where color and brightness need to be treated differently.\n",
        "\n",
        "The basic idea is that all the perceived brightness information is placed in the first \"luminance\" channel, abbreviated \"Y\", while the color information is placed in the other two \"chrominance\" channels, U/V or Cb/Cr. U or Cb can be thought of as measuring the \"blueness\" of a pixel, with 1 being maximum blue, and 0 being the opposite of blue (basically yellow). V or Cr can be thought of as measuring the \"redness\" of a pixel, with 1 being maximum red, and 0 being the opposite of red (i.e. green).\n",
        "\n",
        "![CbCr plane](https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/YCbCr-CbCr_Scaled_Y50.png/480px-YCbCr-CbCr_Scaled_Y50.png)\n",
        "\n",
        "This is useful because the Y channel on its own effectively represents a grayscale version of a color image. This can be useful for color-independent processing, such as if you want to find the shadows or highlights in a color image. Meanwhile, looking at the U/V or Cb/Cr channels alone lets you pick apart different colors in a picture very easily. This is used extensively in color grading movies, and the use of these \"chroma\" channels leads to the popular \"chroma-keying\" or green-screen proceses used in video production and visual effects. We'll explore this more in the next section.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oZWHU1EwqnX4"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "To help visualize YUV, let's take an image and break it apart into its Y, U, and V components. Note that when visualizaing the U and V channels, zeroing out Y channel would just turn the image black. So let's set Y to a fixed value of 0.5, representing half the maximum luminance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdSt2-BYTw_0"
      },
      "outputs": [],
      "source": [
        "yuv_image = skimage.color.rgb2yuv(hubble_image)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Y only\")\n",
        "y_only_image = yuv_image.copy()\n",
        "# Zero out U channel\n",
        "y_only_image[:,:,1] = 0\n",
        "# Zero out V channel\n",
        "y_only_image[:,:,2] = 0\n",
        "plt.imshow(skimage.color.yuv2rgb(y_only_image).clip(0, 1))\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"U only\")\n",
        "u_only_image = yuv_image.copy()\n",
        "# WRITE CODE HERE\n",
        "# Set Y channel to half luminance (0.5)\n",
        "\n",
        "# Zero out V channel\n",
        "\n",
        "# END CODE\n",
        "plt.imshow(skimage.color.yuv2rgb(u_only_image).clip(0, 1))\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"V only\")\n",
        "v_only_image = yuv_image.copy()\n",
        "# WRITE CODE HERE\n",
        "# Set Y channel to half luminance (0.5)\n",
        "\n",
        "# Zero out U channel\n",
        "\n",
        "# END CODE\n",
        "plt.imshow(skimage.color.yuv2rgb(v_only_image).clip(0, 1))\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DemrH8c0rWNp"
      },
      "source": [
        "## 2.4 Color Segmentation\n",
        "\n",
        "One of the most basic tasks in computer vision is figuring out where interesting things are in the image. For instance, a robot might be looking for a target to shoot a ball into or game pieces that it can pick up from the field. This general task of classifying an image into different regions of interest is called image segmentation. The idea is to label each pixel into different classes or segments, each of which belong to a different object of interest or share some useful property.\n",
        "\n",
        "Given how important image segmentation is for computer vision algorithms, it isn't surprising that there exists a wide range of different algorithms covering all kinds of different use cases and techniques. In this section, we will focus on one of the most simple yet effective techniques for image segmentation commonly used today: color based segmentation.\n",
        "\n",
        "The basic idea here is really quite simple. If you know what color the object you are looking for should be, then you can divide the image into pixels that match that color and pixels that don't. The tricky part is turning this into an algorithm that is both accurate and robust to different lighting conditions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eBbwA83buYUu"
      },
      "source": [
        "For the coding exercises, we'll be using color segmentation to perform background subtraction. Commonly known as \"chroma-keying\" or \"green screen,\" the task is to identify the background pixels of a video which has a well defined background color. Then, we can remove only the background pixels and replace them with some other, more interesting background.\n",
        "\n",
        "Foreground image is from \"[#INTRODUCTIONS\" By LaBeouf, Rรถnkkรถ & Turner](https://vimeo.com/125095515), used under CC BY-NC-SA licence. Feel free to add your own background image, but make sure to resize it to 640x360 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q65TslBLzECd"
      },
      "outputs": [],
      "source": [
        "foreground_image = bgr_to_rgb(cv2.imread('repo_files/data/foreground.png'))\n",
        "plt.figure()\n",
        "plt.title(\"Foreground\")\n",
        "plt.imshow(foreground_image)\n",
        "\n",
        "background_image = bgr_to_rgb(cv2.imread('repo_files/data/background.png'))\n",
        "plt.figure()\n",
        "plt.title(\"Background\")\n",
        "plt.imshow(background_image)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MSE9QNgN2eYh"
      },
      "source": [
        "### 2.4.1 Background color model\n",
        "\n",
        "The first step is to come up with a model for what we will consider the background color. First, to avoid issues with varying lighting, let's convert to YUV and only consider the U and V channels. Then, we'll select a section of the image that we know is background (say, the left third of the image) and find the mean and standard deviation of the U and V values in that region. From these statistics, we can model the green color of the background as some mean U,V value, plus/minus some margin of error."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KDQtceD19CcQ"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcCwl69Z28uM"
      },
      "outputs": [],
      "source": [
        "# Convert foreground_image to YUV\n",
        "foreground_yuv = # WRITE CODE HERE\n",
        "\n",
        "# Extract known green pixels, from the left 200 columns of pixels\n",
        "plt.title(\"Known green pixels\")\n",
        "plt.imshow(foreground_image[:,:200])\n",
        "\n",
        "# Convert the known green pixels to YUV\n",
        "known_background_yuv = # WRITE CODE HERE\n",
        "\n",
        "# Reshape into list of pixels, with dimensions (length * width, 3)\n",
        "known_background_yuv = np.reshape(known_background_yuv, (-1, 3))\n",
        "\n",
        "# Find mean and standard deviation. Hint: remember you can pass \"axis\" argument to only average along one dimension of an array\n",
        "mean_color_yuv = # WRITE CODE HERE\n",
        "stddev_color_yuv = # WRITE CODE HERE\n",
        "\n",
        "# Display the mean color\n",
        "plt.figure()\n",
        "plt.title(\"Mean color\")\n",
        "plt.imshow(skimage.color.yuv2rgb(np.full((25, 25, 3), mean_color_yuv)))\n",
        "plt.show()\n",
        "\n",
        "print(\"U standard deviation:\", stddev_color_yuv[1])\n",
        "print(\"V standard deviation:\", stddev_color_yuv[2])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mV88Mk6478JW"
      },
      "source": [
        "### 2.4.2 Segmentation\n",
        "Great, now we can perform actual segmentation. The output of this step will be a \"mask\" array. Each pixel of the mask is simply a integer 0 if that pixel is the same color as the background, or 1 if it isn't. Of course, we want to give some leeway for slight variations in color. So instead of doing a straight equality comparison, we check if the color matches within a tunable margin of error."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YaK1KDZc87UP"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "First, let's define our lower and upper thresholds, as some multiple of the standard deviations for U and V. Remember we want to ignore Y so we aren't affected by changes in lighting. So set the Y threshold to accept any value between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOvYMtgKvQLq"
      },
      "outputs": [],
      "source": [
        "low_threshold = np.array([0.0,\n",
        "                          mean_color_yuv[1] - 10 * stddev_color_yuv[1],\n",
        "                          mean_color_yuv[2] - 10 * stddev_color_yuv[2]])\n",
        "\n",
        "# WRITE CODE HERE\n",
        "high_threshold = \n",
        "# END CODE\n",
        "print(\"Low threshold:\", low_threshold)\n",
        "print(\"High threshold:\", high_threshold)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tamQfcnW97jS"
      },
      "source": [
        "Next let's perform the actual comparison. Remember that for boolean arrays, `a * b` gives the logical and of a and b. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb4OzM0L-kE0"
      },
      "outputs": [],
      "source": [
        "# mask is True where both U and V are between our low and high thresholds\n",
        "background_mask = np.all((foreground_yuv > low_threshold) * (foreground_yuv < high_threshold), axis=2)\n",
        "# Covert mask to integer 1 where True and 0 where False\n",
        "background_mask = np.where(background_mask, 1, 0)\n",
        "# Tack on an extra dimension to make mask shape (360,640,1), so that we can do numpy ops with color images\n",
        "background_mask = np.expand_dims(background_mask, 2)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Masked foreground\")\n",
        "plt.imshow(foreground_image * (1 - background_mask))\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Masked background\")\n",
        "plt.imshow(background_image * background_mask)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "92e_y1VRCkDn"
      },
      "source": [
        "Finally, combine the masked foreground with the masked background image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_c5BEWTCqiL"
      },
      "outputs": [],
      "source": [
        "# Set pixels that are in the foreground (background_mask == 0) to the foreground pixel values\n",
        "# Set pixels that are in the background (background_mask == 1) to the background pixel values\n",
        "combined_image = foreground_image * (1 - background_mask) + background_image * background_mask\n",
        "\n",
        "plt.imshow(combined_image)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "IntroToCV_2_Color.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
